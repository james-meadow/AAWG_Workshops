{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Brief Intro to Natural Langage Processing & Topic Modeling\n",
    "\n",
    "#### AAWG Dev Day 6/14/2019 \n",
    "\n",
    "-----------\n",
    "\n",
    "NLP + LDA are sets of algorithms that help you organize, summarize, and understand large amounts of text data. \n",
    "\n",
    "We're only going to scratch the surface with a simplified view of both concepts, but this has all of the major steps in a topic modeling workflow. There are also some common problems embedded in the workflow below, which we can recognize and fix along the way. \n",
    "\n",
    "First we need the normal data science packages `numpy` and `pandas`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnyMkx6daz6V"
   },
   "outputs": [],
   "source": [
    "import pandas as np\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can use a big text dataset of dubious provenance (`fetch_20newsgroups`). The best description is in the [`sklearn` source code.](https://github.com/scikit-learn/scikit-learn/blob/7813f7efb/sklearn/datasets/twenty_newsgroups.py)\n",
    "\n",
    "We begin by splitting into train and test sets, just to illustrate that `sklearn` has already given us a clean way to do so! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YAhCa2npaz6Z"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "news_train = fetch_20newsgroups(subset='train', shuffle = True)\n",
    "# news_test = fetch_20newsgroups(subset='test', shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the topics available to us in this dataset. This gives us a really nice test dataset to draw clear topic distinctions (autos vs space, for example) or quite similar (hockey vs baseball) to optimize topic model parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "YREPBUDYaz6b",
    "outputId": "3dd805a4-6a46-4954-d8bb-7f142a93ed1c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name in news_train.target_names: print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each \"news\" entry is an email with lots of weird characters and issues. Perfect! Then the next step is to clean using a few very standard techniques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "2fm09TCuaz6f",
    "outputId": "8bb4f074-1cc7-4d67-fc72-13fbd628df4f"
   },
   "outputs": [],
   "source": [
    "news_train.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['rec.sport.baseball', 'comp.graphics']\n",
    "news_train = fetch_20newsgroups(subset='train', \n",
    "                                  categories=categories, \n",
    "                                  shuffle=True, \n",
    "                                  random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First make a nice copy of the data. We can come back to this step every time something goes wrong. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "XwwJo9S8az6j",
    "outputId": "45397ead-624b-4a4c-eb38-f48a5979250d"
   },
   "outputs": [],
   "source": [
    "import unicodedata \n",
    "import sys \n",
    "text = news_train.data[:1000]\n",
    "text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean punctuation\n",
    "\n",
    "All of the symbols and punctuation can go. For our purpose, not very helpful. \n",
    "\n",
    "**What else is happening in here. Also crucial for NLP.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "fpKpokTdaz6o",
    "outputId": "ae8af7cc-bbc1-4c99-a76d-7ba507955459"
   },
   "outputs": [],
   "source": [
    "## Dictionary of all punctuation\n",
    "punctuation = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                        if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "## now we can remove punctuation. \n",
    "text = [string.translate(punctuation).lower() for string in text]\n",
    "# text = text.lower()\n",
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(list(text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize & Remove Stopwords \n",
    "\n",
    "Split up the content using white spaces. Then remove all of the words that just get in thee way. \n",
    "\n",
    "**Which stopwords are we removing below?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dnwQZZedaz6r"
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "\n",
    "## You'll need to download them once before using below. \n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "xio9TV91az6u",
    "outputId": "a36265f5-ce69-4670-d075-666e73e1d34e"
   },
   "outputs": [],
   "source": [
    "tok = [word_tokenize(t) for t in text]\n",
    "tok[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "23cc3JNMaz6x",
    "outputId": "ce9a089e-f5a1-4393-827a-bfc50b818fec"
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aAQqEXIUaz6z"
   },
   "outputs": [],
   "source": [
    "stp = [[word for word in tok_i if word not in stop_words] for tok_i in tok]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the same entry without any stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "gBuAEoLMaz61",
    "outputId": "6b3ea96c-4dd4-4df3-891c-34bb5d24a664"
   },
   "outputs": [],
   "source": [
    "stp[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "'Cook', 'Cooked', 'Cooking', 'Cooker' all have low statistical power separately, but share a root (or stem) meaning, and thus should be considered together for a better model. \n",
    "\n",
    "Thus Stemming is a sometimes off-putting way of getting closer to the root meaning of a word. We simply chop off the last few letters. It gets more elegant that that, but we're really just pruning most of our words to force them to group better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pgYV0uciaz62"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "mln475c-az64",
    "outputId": "81d743d3-32a5-4adb-a76d-63176d0a917f"
   },
   "outputs": [],
   "source": [
    "stem = [[porter.stem(word) for word in entry] for entry in stp]\n",
    "stem[0][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep a clean copy as a backup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BAztSqrWgnFP"
   },
   "outputs": [],
   "source": [
    "processed_text = stem\n",
    "# processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cuG6ZGcjaz67"
   },
   "source": [
    "### LDA Modeling \n",
    "\n",
    "And now to modeling!! pretty simple actually but it takes a minute. We'll use the popular `gensim` package, but there are certainly other options. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vvoA8RyZaz69"
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import re\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check: how big is our dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-j7HN3VAt8CX",
    "outputId": "2d12f178-a258-4c1d-ff1d-4a51cee18621"
   },
   "outputs": [],
   "source": [
    "len(processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary of words in our dataset, and their counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "VFj7C20xjma8",
    "outputId": "6acb9b31-e9f6-48cf-d769-100dea64f4b3"
   },
   "outputs": [],
   "source": [
    "stem_dictionary = corpora.Dictionary(processed_text)\n",
    "stem_corpus = [stem_dictionary.doc2bow(stem) for stem in processed_text]\n",
    "stem_corpus[0][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, build an LDA model and harvest some evaluation information (Coherence is a metric of importance!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9-Y6xdd089g1"
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "  warnings.simplefilter(\"ignore\")\n",
    "\n",
    "  stem_model = models.ldamodel.LdaModel(corpus=stem_corpus, \n",
    "                                        id2word=stem_dictionary, \n",
    "                                        num_topics=2, \n",
    "                                        passes=10, \n",
    "                                        random_state = 1)\n",
    "  stem_cm = CoherenceModel(model=stem_model, \n",
    "                           texts=processed_text, \n",
    "                           dictionary=stem_dictionary, \n",
    "                           coherence='c_v')\n",
    "  stem_coherence = stem_cm.get_coherence()\n",
    "stem_coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out this fabulous visualization for assessing and exploring your model results! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ToJvJ2w4tGS0"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install pyLDAvis\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim as gensimvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "C7J_aVFOtNQ7",
    "outputId": "569dcb5f-5faf-40fc-c95a-df99870038b3"
   },
   "outputs": [],
   "source": [
    "## This visualization might not be supported yet in Colab. \n",
    "## Try it in a jupyter notebook! \n",
    "vis_data = gensimvis.prepare(stem_model, stem_corpus, stem_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "colab_type": "code",
    "id": "DJ5eGSv4k3gK",
    "outputId": "a9f2491a-aa3a-488d-bfc1-e23603eb62ff"
   },
   "outputs": [],
   "source": [
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MYbvCbHTaz6_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hmAuSUH4az7C"
   },
   "source": [
    "### Followup challenges: \n",
    "\n",
    "* Remove the remaining special characters \n",
    "* Package up the NLP workflow into functions for efficient iteration \n",
    "* Use the `test` set to evaluate model fit. We only built a model with the training set -- we didn't actually evaulate it!. \n",
    "* Test implementing lemmatization, and compare the results to those of stemming used in this example \n",
    "* Use other subsets of the data (or the full set) to discover patterns of similarity among topics \n",
    "* How many clusters should we have used? Iterate through model parameters and plot the coherence scores to detect the optimal model fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AAWG_NLP.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
